{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference Recommender for HuggingFace BERT Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Contents\n",
    "[1. Introduction](#1.-Introduction)  \n",
    "[3. Machine Learning model details](#2.-Machine-Learning-model-details)  \n",
    "[4. Register Model Version/Package](#3.-Register-Model-Version/Package)  \n",
    "[5. Create a SageMaker Inference Recommender Default Job](#4:-Create-a-SageMaker-Inference-Recommender-Default-Job)   \n",
    "[6. Instance Recommendation Results](#5.-Instance-Recommendation-Results)   \n",
    "[7. Create an Endpoint for lowest latency real-time inference](#6.-Create-an-Endpoint-for-lowest-latency-real-time-inference)  \n",
    "[8. Clean up](#7.-Clean-up)  \n",
    "[9. Conclusion](#8.-Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "SageMaker Inference Recommender is a new capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating performance benchmarking and load testing models across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time inference endpoint that delivers the best performance at the lowest cost. \n",
    "\n",
    "Get started with Inference Recommender on SageMaker in minutes while selecting an instance and get an optimized endpoint configuration in hours, eliminating weeks of manual testing and tuning time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's update the required packages i.e. SageMaker Python SDK, `boto3`, `botocore` and `awscli`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.183.0)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (1.31.42)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.28.42)\n",
      "Requirement already satisfied: awscli in /opt/conda/lib/python3.10/site-packages (1.29.42)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.25.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore) (1.26.16)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install sagemaker botocore boto3 awscli transformers --upgrade\n",
    "!pip install --upgrade pip awscli botocore boto3  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in SageMaker Studio, you need to make sure `ipywidgets` is installed and restart the kernel, so please uncomment the code in the next cell, and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "\n",
    "!{sys.executable} -m pip install ipywidgets\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download a pre-trained Model\n",
    "\n",
    "In this example, we are using a `Huggingface` pre-trained `sentiment-analysis` model.\n",
    "\n",
    "You can learn more about it in the ðŸ¤— Transformers library Quick tour: https://huggingface.co/docs/transformers/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "Stored 'payload_archive_name' (str)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import pprint\n",
    "import utils\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "\n",
    "payload_archive_name = \"hf_payload.tar.gz\"\n",
    "print(region)\n",
    "%store payload_archive_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "deploy_instance_type                  -> 'ml.m5.xlarge'\n",
      "distilbert_model_name                 -> 'hf-pytorch-model-distilbert-2023-08-25-03-36-14'\n",
      "model_data_path                       -> 's3://sagemaker-us-east-1-805087355833/sagemaker/h\n",
      "model_distilbert_uri                  -> 's3://sagemaker-us-east-1-805087355833/sagemaker/h\n",
      "model_package_group_name              -> 'HuggingFaceModels'\n",
      "model_roberta_script_uri              -> 's3://sagemaker-us-east-1-805087355833/sagemaker/h\n",
      "model_roberta_uri                     -> 's3://sagemaker-us-east-1-805087355833/sagemaker/h\n",
      "payload_archive_name                  -> 'hf_payload.tar.gz'\n",
      "roberta_model_name                    -> 'hf-pytorch-model-roberta-2023-08-25-03-36-14'\n",
      "roberta_script_model_name             -> 'hf-pytorch-model-roberta-script-2023-08-25-03-36-\n"
     ]
    }
   ],
   "source": [
    "%store\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tar the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.csv\n"
     ]
    }
   ],
   "source": [
    "!cd ./sample_payload/ && tar czvf ../{payload_archive_name} test_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the model and payload to S3\n",
    "\n",
    "We now have a model archive and the payload ready. We need to upload it to S3 before we can use it with Inference Recommender, so we will use the SageMaker Python SDK to handle the upload.\n",
    "\n",
    "We need to create an archive that contains individual files that Inference Recommender can send to your SageMaker Endpoints. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-inference-recommender/inference/hf_payload.tar.gz\n",
      "s3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz\n",
      "CPU times: user 129 ms, sys: 3.84 ms, total: 133 ms\n",
      "Wall time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/huggingface-pytorch-inference-recommender\"\n",
    "\n",
    "sample_payload_url = sagemaker.Session().upload_data(\n",
    "    payload_archive_name, bucket=bucket, key_prefix=prefix + \"/inference\"\n",
    ")\n",
    "model_url = model_distilbert_uri\n",
    "\n",
    "print(sample_payload_url)\n",
    "print(model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning model details\n",
    "\n",
    "Inference Recommender uses information about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like and Inference Recommender will use that to provide recommendations.\n",
    "\n",
    "Example ML Domains: `COMPUTER_VISION`, `NATURAL_LANGUAGE_PROCESSING`, `MACHINE_LEARNING`\n",
    "\n",
    "Example ML Tasks: `CLASSIFICATION`, `REGRESSION`, `OBJECT_DETECTION`, `OTHER`\n",
    "\n",
    "Note: Select the task that is the closest match to your model. Chose `OTHER` if none apply.\n",
    "\n",
    "Example Model name: `resnet50`, `yolov4`, `xgboost` etc\n",
    "\n",
    "Use list_model_metadata API to fetch the list of available models. This will help you to pick the closest model for better recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Framework</th>\n",
       "      <th>FrameworkVersion</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>MXNET</td>\n",
       "      <td>1.8.0</td>\n",
       "      <td>densenet201-gluon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>MXNET</td>\n",
       "      <td>1.8.0</td>\n",
       "      <td>resnet18v2-gluon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>resnet152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>efficientnetb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>nasnetlarge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>vgg16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>inception-v3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>xception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>densenet201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>xceptionV1-keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_CLASSIFICATION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>resnet50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_SEGMENTATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>IMAGE_SEGMENTATION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>mask-rcnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>yolov4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>faster-rcnn-resnet101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>COMPUTER_VISION</td>\n",
       "      <td>OBJECT_DETECTION</td>\n",
       "      <td>TENSORFLOW</td>\n",
       "      <td>1.15.5</td>\n",
       "      <td>retinanet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>CLASSIFICATION</td>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>1.0-1</td>\n",
       "      <td>xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>REGRESSION</td>\n",
       "      <td>SAGEMAKER-SCIKIT-LEARN</td>\n",
       "      <td>0.23-1</td>\n",
       "      <td>sagemaker-scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MACHINE_LEARNING</td>\n",
       "      <td>REGRESSION</td>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>1.3-1</td>\n",
       "      <td>xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NATURAL_LANGUAGE_PROCESSING</td>\n",
       "      <td>FILL_MASK</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>bert-base-cased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NATURAL_LANGUAGE_PROCESSING</td>\n",
       "      <td>FILL_MASK</td>\n",
       "      <td>PYTORCH</td>\n",
       "      <td>1.6.0</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Domain                     Task                Framework        FrameworkVersion          Model         \n",
       "9               COMPUTER_VISION  IMAGE_CLASSIFICATION                   MXNET       1.8.0            densenet201-gluon\n",
       "10              COMPUTER_VISION  IMAGE_CLASSIFICATION                   MXNET       1.8.0             resnet18v2-gluon\n",
       "14              COMPUTER_VISION  IMAGE_CLASSIFICATION                 PYTORCH       1.6.0                    resnet152\n",
       "0               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5               efficientnetb7\n",
       "4               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                  nasnetlarge\n",
       "5               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                        vgg16\n",
       "6               COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                 inception-v3\n",
       "11              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                     xception\n",
       "12              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                  densenet201\n",
       "17              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5             xceptionV1-keras\n",
       "18              COMPUTER_VISION  IMAGE_CLASSIFICATION              TENSORFLOW      1.15.5                     resnet50\n",
       "1               COMPUTER_VISION    IMAGE_SEGMENTATION                 PYTORCH       1.6.0                         unet\n",
       "7               COMPUTER_VISION    IMAGE_SEGMENTATION                 PYTORCH       1.6.0                    mask-rcnn\n",
       "13              COMPUTER_VISION      OBJECT_DETECTION                 PYTORCH       1.6.0                       yolov4\n",
       "3               COMPUTER_VISION      OBJECT_DETECTION              TENSORFLOW      1.15.5        faster-rcnn-resnet101\n",
       "19              COMPUTER_VISION      OBJECT_DETECTION              TENSORFLOW      1.15.5                    retinanet\n",
       "2              MACHINE_LEARNING        CLASSIFICATION                 XGBOOST       1.0-1                      xgboost\n",
       "8              MACHINE_LEARNING            REGRESSION  SAGEMAKER-SCIKIT-LEARN      0.23-1       sagemaker-scikit-learn\n",
       "15             MACHINE_LEARNING            REGRESSION                 XGBOOST       1.3-1                      xgboost\n",
       "16  NATURAL_LANGUAGE_PROCESSING             FILL_MASK                 PYTORCH       1.6.0              bert-base-cased\n",
       "20  NATURAL_LANGUAGE_PROCESSING             FILL_MASK                 PYTORCH       1.6.0            bert-base-uncased"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "list_model_metadata_response = client.list_model_metadata()\n",
    "\n",
    "domains = []\n",
    "frameworks = []\n",
    "framework_versions = []\n",
    "tasks = []\n",
    "models = []\n",
    "\n",
    "for model_summary in list_model_metadata_response[\"ModelMetadataSummaries\"]:\n",
    "    domains.append(model_summary[\"Domain\"])\n",
    "    tasks.append(model_summary[\"Task\"])\n",
    "    models.append(model_summary[\"Model\"])\n",
    "    frameworks.append(model_summary[\"Framework\"])\n",
    "    framework_versions.append(model_summary[\"FrameworkVersion\"])\n",
    "\n",
    "data = {\n",
    "    \"Domain\": domains,\n",
    "    \"Task\": tasks,\n",
    "    \"Framework\": frameworks,\n",
    "    \"FrameworkVersion\": framework_versions,\n",
    "    \"Model\": models,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.colheader_justify\", \"center\")\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "\n",
    "display(df.sort_values(by=[\"Domain\", \"Task\", \"Framework\", \"FrameworkVersion\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, as we are predicting Sentiment analysis with `HuggingFace` `BERT`, we select `NATURAL_LANGUAGE_PROCESSING` as the Domain, `FILL_MASK` as the Task, `PYTORCH` as the Framework, and `bert-base-uncased` as the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_domain = \"NATURAL_LANGUAGE_PROCESSING\"\n",
    "ml_task = \"FILL_MASK\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.6.0\"\n",
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container image URL\n",
    "\n",
    "If you donâ€™t have an inference container image, you can use [Prebuilt SageMaker Docker Images for Deep Learning](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) provided by AWS to serve your ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.10.2-cpu-py38\n"
     ]
    }
   ],
   "source": [
    "# ML model details\n",
    "model_name = \"huggingface-pytorch-\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "inference_image = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.10.2\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "print(inference_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create SageMaker Model\n",
    "** Mel to review for description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name=\"HuggingFace-PyTorch-Inference-Recommender-Demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            'ContainerHostname': \"huggingface-pytorch\",\n",
    "            'Image': inference_image,\n",
    "            'Mode': 'SingleModel',\n",
    "            'ModelDataUrl': model_url, #should be S3 path to model data\n",
    "            'Environment': {\n",
    "                 \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "                    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "                    \"SAGEMAKER_REGION\": \"us-east-1\",\n",
    "                    \"SAGEMAKER_SUBMIT_DIRECTORY\": model_url #should be S3 path to model data\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    ExecutionRoleArn=role,\n",
    "    EnableNetworkIsolation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Create a SageMaker Inference Recommender Default Job\n",
    "\n",
    "Now with your model you can kick off a 'Default' job to get instance recommendations.\n",
    "\n",
    "As `SamplePayloadUrl` and `SupportedContentTypes` parameters are essential for benchmarking the endpoint, we also highly recommend that you specify `Domain`, `Task`, `Framework`, `FrameworkVersion`, `NearestModelName` for better inference recommendation.\n",
    "\n",
    "The output is a list of instance type recommendations with associated environment variables, cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-1:805087355833:inference-recommendations-job/huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'ResponseMetadata': {'RequestId': '67f2226c-fb1d-405f-b450-0ab28ebfab23', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '67f2226c-fb1d-405f-b450-0ab28ebfab23', 'content-type': 'application/x-amz-json-1.1', 'content-length': '145', 'date': 'Thu, 07 Sep 2023 01:08:49 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "default_job = \"huggingface-pytorch-basic-recommender-job-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\"\n",
    ")\n",
    "default_response = client.create_inference_recommendations_job(\n",
    "    JobName=str(default_job),\n",
    "    JobDescription=\"HuggingFace PyTorch Inference Basic Recommender Job\",\n",
    "    JobType=\"Default\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelName\": model_name, \n",
    "                  'ContainerConfig': {\n",
    "            'Domain': ml_domain,\n",
    "            'Task': ml_task,\n",
    "            'Framework': ml_framework,\n",
    "            'FrameworkVersion': framework_version,\n",
    "            'PayloadConfig': {\n",
    "                'SamplePayloadUrl': sample_payload_url,\n",
    "                'SupportedContentTypes': [\"text/csv\"]\n",
    "            },\n",
    "            'NearestModelName': model,\n",
    "            'SupportedInstanceTypes': [\n",
    "                \"ml.c5.xlarge\",\n",
    "                \"ml.c5.2xlarge\"\n",
    "            ],\n",
    "            'SupportedResponseMIMETypes': [\"text/csv\"]\n",
    "        },\n",
    "                },\n",
    "    \n",
    "  \n",
    ")\n",
    "\n",
    "print(default_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Instance Recommendation Results\n",
    "\n",
    "The inference recommender job provides multiple endpoint recommendations in its result. The recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which includes tuned parameters for better performance. We also include the benchmarking results like `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference` for deeper analysis. The information provided will help you narrow down to a specific endpoint configuration that suits your use case.\n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is latency/throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Inference recommender job will take ~25 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job in progress\n",
      "Inference recommender job completed\n",
      "CPU times: user 447 ms, sys: 41 ms, total: 488 ms\n",
      "Wall time: 30min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ended = False\n",
    "while not ended:\n",
    "    inference_recommender_job = client.describe_inference_recommendations_job(\n",
    "        JobName=str(default_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        ended = True\n",
    "    else:\n",
    "        print(\"Inference recommender job in progress\")\n",
    "        time.sleep(60)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the job status from the Inference Recommender console. In the navigation pane, under **Home**, **Deployments**, select **Inference recommender**.\n",
    "<div>\n",
    "    <img src=\"./images/IR1.png\" alt=\"Image IR1\" width=\"300\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This will open a new tab showing all the Inference Recommender (IR) jobs.\n",
    "<div>\n",
    "    <img src=\"./images/IR2.png\" alt=\"Image IR2\" width=\"800\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Once the job finishes, you can click into the job and see the summary of model performance using different instances and the job details.\n",
    "<div>\n",
    "    <img src=\"./images/IR3.png\" alt=\"Image IR3\" width=\"800\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "dropFilter = df.filter([\"VariantName\"])\n",
    "df.drop(dropFilter, inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the result `dataframe` by `MaxInvocations` - The maximum number of requests per minute expected for the endpoint, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ServerlessConfig</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "      <th>MemoryUtilization</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>CpuUtilization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '8'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...</td>\n",
       "      <td>0.408</td>\n",
       "      <td>4.613e-06</td>\n",
       "      <td>1474</td>\n",
       "      <td>416</td>\n",
       "      <td>27.775</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>773.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '4'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...</td>\n",
       "      <td>0.204</td>\n",
       "      <td>4.404e-06</td>\n",
       "      <td>772</td>\n",
       "      <td>605</td>\n",
       "      <td>30.156</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q</td>\n",
       "      <td>{'MemorySizeInMB': 6144, 'MaxConcurrency': 1}</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1.452e-05</td>\n",
       "      <td>496</td>\n",
       "      <td>95</td>\n",
       "      <td>13.412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj</td>\n",
       "      <td>{'MemorySizeInMB': 5120, 'MaxConcurrency': 1}</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...</td>\n",
       "      <td>0.360</td>\n",
       "      <td>1.274e-05</td>\n",
       "      <td>471</td>\n",
       "      <td>91</td>\n",
       "      <td>15.187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           EndpointName                                         ServerlessConfig                                                                                                                                                                                                             EnvironmentParameters                                                                                                                                                                                                CostPerHour  CostPerInference  MaxInvocations  ModelLatency  MemoryUtilization  InstanceType   InitialInstanceCount  CpuUtilization\n",
       "3  huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf                                            NaN  [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '8'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...     0.408         4.613e-06          1474            416           27.775        ml.c5.2xlarge           1.0              773.462   \n",
       "1  huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre                                            NaN  [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '4'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...     0.204         4.404e-06           772            605           30.156         ml.c5.xlarge           1.0              384.154   \n",
       "2  huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q  {'MemorySizeInMB': 6144, 'MaxConcurrency': 1}  [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...     0.432         1.452e-05           496             95           13.412                  NaN           NaN                  NaN   \n",
       "0  huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj  {'MemorySizeInMB': 5120, 'MaxConcurrency': 1}  [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...     0.360         1.274e-05           471             91           15.187                  NaN           NaN                  NaN   "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"MaxInvocations\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's sort the result `dataframe` by `ModelLatencyThresholds` - The interval of time taken by a model to respond as viewed from SageMaker. The interval includes the local communication time taken to send the request and to fetch the response from the container of a model and the time taken to complete the inference in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ServerlessConfig</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "      <th>MemoryUtilization</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>CpuUtilization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj</td>\n",
       "      <td>{'MemorySizeInMB': 5120, 'MaxConcurrency': 1}</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...</td>\n",
       "      <td>0.360</td>\n",
       "      <td>1.274e-05</td>\n",
       "      <td>471</td>\n",
       "      <td>91</td>\n",
       "      <td>15.187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q</td>\n",
       "      <td>{'MemorySizeInMB': 6144, 'MaxConcurrency': 1}</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1.452e-05</td>\n",
       "      <td>496</td>\n",
       "      <td>95</td>\n",
       "      <td>13.412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '8'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...</td>\n",
       "      <td>0.408</td>\n",
       "      <td>4.613e-06</td>\n",
       "      <td>1474</td>\n",
       "      <td>416</td>\n",
       "      <td>27.775</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>773.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '4'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...</td>\n",
       "      <td>0.204</td>\n",
       "      <td>4.404e-06</td>\n",
       "      <td>772</td>\n",
       "      <td>605</td>\n",
       "      <td>30.156</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384.154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           EndpointName                                         ServerlessConfig                                                                                                                                                                                                             EnvironmentParameters                                                                                                                                                                                                CostPerHour  CostPerInference  MaxInvocations  ModelLatency  MemoryUtilization  InstanceType   InitialInstanceCount  CpuUtilization\n",
       "0  huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj  {'MemorySizeInMB': 5120, 'MaxConcurrency': 1}  [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...     0.360         1.274e-05           471             91           15.187                  NaN           NaN                  NaN   \n",
       "2  huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q  {'MemorySizeInMB': 6144, 'MaxConcurrency': 1}  [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String...     0.432         1.452e-05           496             95           13.412                  NaN           NaN                  NaN   \n",
       "3  huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf                                            NaN  [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '8'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...     0.408         4.613e-06          1474            416           27.775        ml.c5.2xlarge           1.0              773.462   \n",
       "1  huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre                                            NaN  [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '4'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'Str...     0.204         4.404e-06           772            605           30.156         ml.c5.xlarge           1.0              384.154   "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"ModelLatency\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MemorySizeInMB': 5120, 'MaxConcurrency': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if Serverless is the best option, execute this cell.\n",
    "ServerlessConfig = df.sort_values(by=[\"ModelLatency\"]).head()['ServerlessConfig'][0]\n",
    "ServerlessMem= df.sort_values(by=[\"ModelLatency\"]).head()['ServerlessConfig'][0]['MemorySizeInMB']\n",
    "ServerlessConc = df.sort_values(by=[\"ModelLatency\"]).head()['ServerlessConfig'][0]['MaxConcurrency']\n",
    "ServerlessConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the instance with the lowest `ModelLatency`. This is done by choosing the first record of the result `dataframe`, sorted by ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NaN'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type = (\n",
    "    df.sort_values(by=[\"ModelLatency\"]).head(1)[\"InstanceType\"].to_string(index=False).strip()\n",
    ")\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: ListInferenceRecommendationsJobSteps\n",
    "To see the list of subtasks for an Inference Recommender job, simply provide the `JobName` to the `ListInferenceRecommendationsJobSteps` API. \n",
    "\n",
    "To see more information for the API, please refer to the doc here: [ListInferenceRecommendationsJobSteps](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ListInferenceRecommendationsJobSteps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Steps': [{'StepType': 'BENCHMARK', 'JobName': 'huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'Status': 'FAILED', 'InferenceBenchmark': {'EndpointConfiguration': {'EndpointName': 'huggingface-pytorch-basic-recommender-jo-K1Zffkn3jUchQQsPU8dj', 'VariantName': 'huggingface-pytorch-basic-recommender-jo-K1Zffkn3jUchQQsPU8dj', 'ServerlessConfig': {'MemorySizeInMB': 4096, 'MaxConcurrency': 1}}, 'ModelConfiguration': {'EnvironmentParameters': [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String', 'Value': 'inference.py'}]}, 'FailureReason': 'An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message \"Your invocation timed out while waiting for a response from model container. Review the latency metrics in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-basic-recommender-jo-K1Zffkn3jUchQQsPU8dj in account 805087355833 for more information.'}}, {'StepType': 'BENCHMARK', 'JobName': 'huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'Status': 'COMPLETED', 'InferenceBenchmark': {'Metrics': {'CostPerHour': 0.40799999237060547, 'CostPerInference': 4.613297278410755e-06, 'MaxInvocations': 1474, 'ModelLatency': 416, 'CpuUtilization': 773.4619750976562, 'MemoryUtilization': 27.775400161743164}, 'EndpointConfiguration': {'EndpointName': 'huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf', 'VariantName': 'huggingface-pytorch-basic-recommender-jo-WVxO9dO0uFSNN7JaGLRf', 'InstanceType': 'ml.c5.2xlarge', 'InitialInstanceCount': 1}, 'ModelConfiguration': {'EnvironmentParameters': [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '8'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String', 'Value': 'inference.py'}]}, 'InvocationEndTime': datetime.datetime(2023, 9, 7, 1, 32, 37, 407000, tzinfo=tzlocal()), 'InvocationStartTime': datetime.datetime(2023, 9, 7, 1, 16, 26, 314000, tzinfo=tzlocal())}}, {'StepType': 'BENCHMARK', 'JobName': 'huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'Status': 'COMPLETED', 'InferenceBenchmark': {'Metrics': {'CostPerHour': 0.4320000112056732, 'CostPerInference': 1.4516129340336192e-05, 'MaxInvocations': 496, 'ModelLatency': 95, 'MemoryUtilization': 13.41165542602539}, 'EndpointConfiguration': {'EndpointName': 'huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q', 'VariantName': 'huggingface-pytorch-basic-recommender-jo-YAREG1RJh4k1GLaeaE0q', 'ServerlessConfig': {'MemorySizeInMB': 6144, 'MaxConcurrency': 1}}, 'ModelConfiguration': {'EnvironmentParameters': [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String', 'Value': 'inference.py'}]}, 'InvocationEndTime': datetime.datetime(2023, 9, 7, 1, 28, 48, 305000, tzinfo=tzlocal()), 'InvocationStartTime': datetime.datetime(2023, 9, 7, 1, 17, 1, 147000, tzinfo=tzlocal())}}, {'StepType': 'BENCHMARK', 'JobName': 'huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'Status': 'COMPLETED', 'InferenceBenchmark': {'Metrics': {'CostPerHour': 0.20399999618530273, 'CostPerInference': 4.404144874570193e-06, 'MaxInvocations': 772, 'ModelLatency': 605, 'CpuUtilization': 384.15399169921875, 'MemoryUtilization': 30.15570068359375}, 'EndpointConfiguration': {'EndpointName': 'huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre', 'VariantName': 'huggingface-pytorch-basic-recommender-jo-RUHE9zc7rHHFOC0aGLre', 'InstanceType': 'ml.c5.xlarge', 'InitialInstanceCount': 1}, 'ModelConfiguration': {'EnvironmentParameters': [{'Key': 'SAGEMAKER_MODEL_SERVER_WORKERS', 'ValueType': 'String', 'Value': '4'}, {'Key': 'OMP_NUM_THREADS', 'ValueType': 'String', 'Value': '1'}, {'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String', 'Value': 'inference.py'}]}, 'InvocationEndTime': datetime.datetime(2023, 9, 7, 1, 29, 42, 906000, tzinfo=tzlocal()), 'InvocationStartTime': datetime.datetime(2023, 9, 7, 1, 16, 32, 141000, tzinfo=tzlocal())}}, {'StepType': 'BENCHMARK', 'JobName': 'huggingface-pytorch-basic-recommender-job-2023-09-07-01-08-45', 'Status': 'COMPLETED', 'InferenceBenchmark': {'Metrics': {'CostPerHour': 0.36000001430511475, 'CostPerInference': 1.2738853911287151e-05, 'MaxInvocations': 471, 'ModelLatency': 91, 'MemoryUtilization': 15.186549186706543}, 'EndpointConfiguration': {'EndpointName': 'huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj', 'VariantName': 'huggingface-pytorch-basic-recommender-jo-K3RXOyxuTuGifLjbfjAj', 'ServerlessConfig': {'MemorySizeInMB': 5120, 'MaxConcurrency': 1}}, 'ModelConfiguration': {'EnvironmentParameters': [{'Key': 'SAGEMAKER_SUBMIT_DIRECTORY', 'ValueType': 'String', 'Value': 's3://sagemaker-us-east-1-805087355833/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz'}, {'Key': 'SAGEMAKER_CONTAINER_LOG_LEVEL', 'ValueType': 'String', 'Value': '20'}, {'Key': 'SAGEMAKER_REGION', 'ValueType': 'String', 'Value': 'us-east-1'}, {'Key': 'SAGEMAKER_PROGRAM', 'ValueType': 'String', 'Value': 'inference.py'}]}, 'InvocationEndTime': datetime.datetime(2023, 9, 7, 1, 27, 21, 825000, tzinfo=tzlocal()), 'InvocationStartTime': datetime.datetime(2023, 9, 7, 1, 17, 1, 125000, tzinfo=tzlocal())}}], 'ResponseMetadata': {'RequestId': 'bd9aeb5b-5030-4f73-b522-238c2ac597b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'bd9aeb5b-5030-4f73-b522-238c2ac597b0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '6018', 'date': 'Thu, 07 Sep 2023 03:52:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "list_job_steps_response = client.list_inference_recommendations_job_steps(JobName=str(default_job))\n",
    "print(list_job_steps_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create an Endpoint for lowest latency real-time inference\n",
    "\n",
    "Next we will create a SageMaker real-time endpoint using the instance with the lowest latency for the model, detected in the Inference Recommender Default Job that was run previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Config from the model\n",
    "\n",
    "This will create an endpoint configuration that Amazon SageMaker hosting services uses to deploy models. In the configuration, you identify one or more models, created using the `CreateModel` API, to deploy and the resources that you want Amazon SageMaker to provision. Then you call the `CreateEndpoint` API.\n",
    "\n",
    "More info on `create_endpoint_config` can be found on the [Boto3 SageMaker documentation page](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:805087355833:endpoint-config/huggingface-pytorch-endpoint-config-2023-09-07-03-52-55',\n",
       " 'ResponseMetadata': {'RequestId': '8c659222-6d0a-4852-b153-b89962cf4aaf',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8c659222-6d0a-4852-b153-b89962cf4aaf',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '136',\n",
       "   'date': 'Thu, 07 Sep 2023 03:52:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = \"huggingface-pytorch-endpoint-config-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\"\n",
    ")\n",
    "\n",
    "# Serverless Deployment\n",
    "if instance_type == 'NaN':\n",
    "        endpoint_config_response = client.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=[\n",
    "                    {\n",
    "                        \"ModelName\": model_name,\n",
    "                        \"VariantName\": \"AllTraffic\",\n",
    "                        \"ServerlessConfig\": {\n",
    "                            \"MemorySizeInMB\": ServerlessMem,\n",
    "                            \"MaxConcurrency\": ServerlessConc,\n",
    "                        }\n",
    "                    } \n",
    "                ]\n",
    "        )\n",
    "# Real time deployment\n",
    "else:\n",
    "    endpoint_config_response = client.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    \"VariantName\": \"AllTrafficVariant\",\n",
    "                    \"ModelName\": model_name,\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"InstanceType\": instance_type,\n",
    "                    \"InitialVariantWeight\": 1,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Endpoint Config to a real-time endpoint\n",
    "\n",
    "This will create an endpoint using the endpoint configuration specified in the request. Amazon SageMaker uses the endpoint to provision resources and deploy models. Note that you have already created the endpoint configuration with the `CreateEndpointConfig` API in the previous step.\n",
    "\n",
    "More info on `create_endpoint` can be found on the [Boto3 SageMaker documentation page](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:805087355833:endpoint/huggingface-pytorch-endpoint-2023-09-07-03-53-01',\n",
       " 'ResponseMetadata': {'RequestId': '9d866d1d-dfce-42d7-bcbe-7a379782932a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9d866d1d-dfce-42d7-bcbe-7a379782932a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '116',\n",
       "   'date': 'Thu, 07 Sep 2023 03:53:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"huggingface-pytorch-endpoint-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\"\n",
    ")\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "\n",
    "create_endpoint_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Endpoint to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "CPU times: user 53.4 ms, sys: 11.2 ms, total: 64.6 ms\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'huggingface-pytorch-endpoint-2023-09-07-03-53-01',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:805087355833:endpoint/huggingface-pytorch-endpoint-2023-09-07-03-53-01',\n",
       " 'EndpointConfigName': 'huggingface-pytorch-endpoint-config-2023-09-07-03-52-55',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.10.2-cpu-py38',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference@sha256:ad5678a4a59d69692e6dff42ce268436f13d0cbcf4dc96b910a9e44949b3c596',\n",
       "     'ResolutionTime': datetime.datetime(2023, 9, 7, 3, 53, 2, 504000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 0,\n",
       "   'CurrentServerlessConfig': {'MemorySizeInMB': 5120, 'MaxConcurrency': 1}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2023, 9, 7, 3, 53, 1, 652000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 9, 7, 3, 55, 21, 257000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'e02137e1-04c7-4cbe-bf16-2bf6e97fcf79',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e02137e1-04c7-4cbe-bf16-2bf6e97fcf79',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '850',\n",
       "   'date': 'Thu, 07 Sep 2023 03:55:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "utils.endpoint_creation_wait(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Endpoint with `boto3`\n",
    "\n",
    "After you deploy a model into production using Amazon SageMaker hosting services, your client applications use this API to get inferences from the model hosted at the specified endpoint.\n",
    "\n",
    "For an overview of Amazon SageMaker, [see How It Works](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works.html).\n",
    "\n",
    "Amazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n",
    "\n",
    "Calls to `InvokeEndpoint` are authenticated by using AWS Signature Version 4. For information, see Authenticating Requests (AWS Signature Version 4) in the Amazon S3 API Reference.\n",
    "\n",
    "A customer's model containers must respond to requests within 60 seconds. The model itself can have a maximum processing time of 60 seconds before responding to invocations. If your model is going to take 50-60 seconds of processing time, the SDK socket timeout should be set to be 70 seconds.\n",
    "\n",
    "More info on `invoke_endpoint` can be found on the [Boto3 `SageMakerRuntime` documentation page](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./sample_payload/test_data.csv\", header=None)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=test_data.to_csv(header=False, index=False),\n",
    "    ContentType=\"text/csv\",\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '84d8cc36-ca00-4c72-880e-3db75b3b4420',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '84d8cc36-ca00-4c72-880e-3db75b3b4420',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 30 Aug 2023 00:32:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook you successfully downloaded a `Huggingface` pre-trained `sentiment-analysis` model, you compressed the `model` and the payload and upload it to Amazon S3. \n",
    "Then you created a SageMaker model, and triggered a SageMaker Inference Recommender Default Job.\n",
    "\n",
    "You then browsed the results, sorted by `MaxInvocations` and by `ModelLatency`, and decided to create an Endpoint for the lowest latency real-time inference.\n",
    "After deploying the model to an endpoint, you invoked the Endpoint with a sample payload of few sentences, using `boto3`, and got the predictions result.\n",
    "\n",
    "As next steps, you can try running SageMaker Inference Recommender on your own models, to select an instance with the best price performance for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
