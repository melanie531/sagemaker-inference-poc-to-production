{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad718ed6-890f-4a3d-a6c4-9eb29d8f92e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A/B Testing with Amazon SageMaker\n",
    "\n",
    "***\n",
    "This notebooks is designed to run on `Python 3 (Data Science 2.0)` kernel in Amazon SageMaker Studio\n",
    "***\n",
    "\n",
    "In production ML workflows, data scientists and data engineers frequently try to improve their models in various ways, such as by performing [Perform Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html), training on additional or more-recent data, and improving feature selection. Performing A/B testing between a new model and an old model with production traffic can be an effective final step in the validation process for a new model. In A/B testing, you test different variants of your models and compare how each variant performs relative to each other. You then choose the best-performing model to replace a previously-existing model new version delivers better performance than the previously-existing version.\n",
    "\n",
    "Amazon SageMaker enables you to test multiple models or model versions behind the same endpoint using production variants. Each production variant identifies a machine learning (ML) model and the resources deployed for hosting the model. You can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant, or you can invoke a specific variant directly for each request.\n",
    "\n",
    "In this notebook, we'll:\n",
    "* Evaluate models by invoking specific variants\n",
    "* Gradually release a new model by specifying traffic distribution\n",
    "\n",
    "Reference notebook example: [A/B Testing with Amazon SageMaker](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_endpoints/a_b_testing/a_b_testing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce61ca-dfb0-455f-afb4-9a28fac22b84",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's set up some required imports and basic initial variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc46aed-816f-4021-998e-c4732bcf2d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers ipywidgets sagemaker torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96189bf5-9d67-4da7-88bd-9b33aa183225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "import time\n",
    "import os, sys\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, image_uris\n",
    "import shutil\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import torch\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "# p = os.path.abspath('..')\n",
    "# if p not in sys.path:\n",
    "#     sys.path.append(p)\n",
    "import utils\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sm_session.boto_region_name\n",
    "bucket = sm_session.default_bucket()\n",
    "sm_client = sm_session.sagemaker_client\n",
    "sm_runtime = sm_session.sagemaker_runtime_client\n",
    "prefix = \"sagemaker/huggingface-pytorch-sentiment-analysis\"\n",
    "time_now = f'{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "time_now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ff699-d9fb-47ba-8a6d-c3d24c06b7c7",
   "metadata": {},
   "source": [
    "### Useful objects and variables\n",
    "Common objects to interact with SageMaker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f90327-aab1-40d2-aef1-99f22098271a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "sm_client = sm_session.sagemaker_client\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "prefix = \"sagemaker/huggingface-pytorch-sentiment-analysis\"\n",
    "deploy_instance_type = \"ml.m5.xlarge\"\n",
    "%store deploy_instance_type\n",
    "\n",
    "# The name of the Model Package Group in Amazon SageMaker Model Registry\n",
    "model_package_group_name = \"HuggingFaceModels\"\n",
    "%store model_package_group_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bf03c-2c80-49c6-9125-c42e3fd55c86",
   "metadata": {},
   "source": [
    "## Step 0: Download HuggingFace Transformer models and Create SageMaker models\n",
    "\n",
    "#### twitter-roberta-base-sentiment Pretrained Model\n",
    "\n",
    "In this example we are downloading a pre-trained HuggingFace model - `twitter-roberta-base-sentiment` from the HuggingFace library. We will use this model for classifying the text as `Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee92bb-6173-4c4d-a7ad-d17a24cf03b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_MODEL_ROBERTA = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_ROBERTA)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ROBERTA)\n",
    "model.save_pretrained(\"model_token_roberta\")\n",
    "tokenizer.save_pretrained(\"model_token_roberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e56457-ff5c-434c-adb2-2d4974040743",
   "metadata": {},
   "source": [
    "### #Package the saved model to tar.gz format\n",
    "Once the model is downloaded, we need to package (tokenizer and model weights) it to `.tar.gz` format as expected by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718ea4f-0051-4e6f-9029-282ae7eb15d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar_file_roberta = \"model_roberta.tar.gz\"\n",
    "tar_size = utils.create_tar(tar_file_roberta, Path(\"model_token_roberta\"))\n",
    "print(f\"Created {tar_file_roberta}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0476c0-47d3-4ba6-8dc9-61ae7ace09c8",
   "metadata": {},
   "source": [
    "#### Download distilbert-base-uncased-finetuned-sst-2-english by initiating a `Huggingface pipeline`\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the [task summary](https://huggingface.co/transformers/task_summary.html) for examples of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2869b-baf6-4d31-88d6-050263dc3e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_MODEL_DISTILBERT = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "HF_TASK = \"sentiment-analysis\"\n",
    "local_artifact_path = Path(\"model_token_distilbert\")\n",
    "local_artifact_path.mkdir(exist_ok=True, parents=True)\n",
    "tar_file_distilbert = \"model_distilbert.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc00f35-1bb5-4c4c-81af-cb80072b10d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(HF_TASK, model=HF_MODEL_DISTILBERT)\n",
    "sentiment_analysis.save_pretrained(local_artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63235936-2123-4bee-a8d1-6b564e787665",
   "metadata": {},
   "source": [
    "#### Write the Inference Script\n",
    "\n",
    "To deploy a pretrained `PyTorch` model, you'll need to use the `PyTorch` estimator object to create a `PyTorchModel` object and set a different `entry_point`.\n",
    "\n",
    "You'll use the `PyTorchModel` object to deploy a `PyTorchPredictor`. This creates a `SageMaker` Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of `model_fn` is required for inference script. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `model_fn` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "Here's an example of the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1e6bf-267b-4f44-9f1c-5340930d35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ../code/inference.py  # uncomment this line of code to see the details in the py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588e608-5e4c-4a0c-9852-ab90c6ede378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ../code/requirements.txt  # uncomment this line to show the packages defined in the requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f05650-6836-4ad8-a2f4-bfe2d1e66369",
   "metadata": {},
   "source": [
    "#### Create the directory structure for your model files\n",
    "\n",
    "The directory structure where you saved your PyTorch model should look something like the following:\n",
    "\n",
    "```\n",
    "|   model\n",
    "|        |--pytorch_model.bin\n",
    "|        |--config.json\n",
    "|        |--vocab.txt\n",
    "|        |--tokenizer.json\n",
    "|        |--tokenizer_config.json\n",
    "|        |--special_tokens_map.json\n",
    "|\n",
    "|        code\n",
    "|            |--inference.py\n",
    "|            |--requirements.txt\n",
    "```\n",
    "\n",
    "Where `requirements.txt` is an optional file that specifies dependencies on third-party libraries.\n",
    "\n",
    "#### Copy code to the model directory and tar the model and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b03198-0112-4598-9a0d-8cfa9811e3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.copytree(\"./code\", \"model_token_distilbert/code\", dirs_exist_ok=True)\n",
    "tar_size =utils.create_tar(tar_file_distilbert, local_artifact_path)\n",
    "print(f\"Created {tar_file_distilbert}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50660e2c-8463-49fb-be66-40aa13e6dbf6",
   "metadata": {},
   "source": [
    "#### Upload the model to S3\n",
    "\n",
    "We now have the model archives ready. We need to upload them to S3 before we can use them for hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73db5f-e0ac-4b2e-a0b1-56b0feb82db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_path = s3_path_join(\"s3://\", bucket, prefix + \"/models\")\n",
    "print(f\"Uploading Models to {model_data_path}\")\n",
    "model_roberta_uri = S3Uploader.upload(\"model_roberta.tar.gz\", model_data_path)\n",
    "print(f\"Uploaded roberta model to {model_roberta_uri}\")\n",
    "model_distilbert_uri = S3Uploader.upload(\"model_distilbert.tar.gz\", model_data_path)\n",
    "print(f\"Uploaded distilbert model to {model_distilbert_uri}\")\n",
    "%store model_data_path\n",
    "%store model_roberta_uri\n",
    "%store model_distilbert_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ee584-ece9-4b02-8c74-00e41927f49f",
   "metadata": {},
   "source": [
    "#### Prebuilt HuggingFace DLC\n",
    "You can choose to use a prebuilt HuggingFace DLC as the inference image, which has the [SageMaker huggingface inference toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) for serving 🤗 Transformers models on Amazon SageMaker. The inference toolkit leverages the pipeline for the transformer library to allow zero-code deployments of models, without requiring any code for pre- or post-processing. (see more information of the default [handler service](https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/src/sagemaker_huggingface_inference_toolkit/handler_service.py) provided bythe inference toolkit).\n",
    "\n",
    "In addition to zero-code deployment, the Inference Toolkit supports \"bring your own code\" methods, where you can override the default methods. You can learn more about \"bring your own code\" in the documentation [here](https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules). In the second lab section, we will use the bring your own code method to deploy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4df9a-af96-4078-8dcf-b564765ff6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "framework = \"huggingface\"\n",
    "transformer_version = \"4.17.0\"\n",
    "py_version = \"py38\"\n",
    "instance_type = \"ml.g\"\n",
    "image_scope = \"inference\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.10.2\"\n",
    "\n",
    "inference_image_roberta = image_uris.retrieve(\n",
    "    framework=framework,\n",
    "    base_framework_version=ml_framework.lower() + framework_version,\n",
    "    region=region,\n",
    "    version=transformer_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=instance_type,\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410f067-8dbb-403a-9090-3bb24cc8afe5",
   "metadata": {},
   "source": [
    "#### Prebuilt Pytorch DLC\n",
    "You can also use a SageMaker prebuilt [Pytorch DLC](https://github.com/aws/deep-learning-containers/tree/master/pytorch) to deploy the huggingface model. In this case, as the prebuilt Pytorch container doesn't have the transformer package, we have provided a `requirements.txt` file with the additional packages that are required to be installed to the container in the model package. See section [Create the directory structure for your model files](#Create-the-directory-structure-for-your-model-files). We also included the `inference.py` file to define the necessary functions for model loading and model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759444c-e12e-494c-95e7-de79ac0bcb35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_distilbert = image_uris.retrieve(\n",
    "    framework=ml_framework.lower(),\n",
    "    region=region,\n",
    "    version=framework_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=instance_type,\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20165a55-e170-404f-a2df-8bde21b6d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# provide the consistent time stamp for model, endpoint config and endpoint\n",
    "now_roberta = f\"{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(now_roberta)\n",
    "roberta_model_name = f\"hf-pytorch-model-roberta-{now_roberta}\"\n",
    "print(\"Model name : {}\".format(roberta_model_name))\n",
    "%store roberta_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfea9c-a48d-4c66-8ed0-991d7a1c58b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primary_container = {\n",
    "    'Image': inference_image_roberta,\n",
    "    'ModelDataUrl': model_roberta_uri\n",
    "}\n",
    "create_roberta_model_response = sm_client.create_model(\n",
    "    ModelName=roberta_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b047bf9-000c-4c13-a693-2ddb773dccc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now_distilbert = f\"{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(now_distilbert)\n",
    "distilbert_model_name = f\"hf-pytorch-model-distilbert-{now_distilbert}\"\n",
    "print(\"Model name : {}\".format(distilbert_model_name))\n",
    "%store distilbert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877adaa-87c7-46ce-a564-e39fe2306182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primary_container = {\n",
    "    'Image': inference_image_distilbert,\n",
    "    'ModelDataUrl': model_distilbert_uri\n",
    "}\n",
    "create_roberta_model_response = sm_client.create_model(\n",
    "    ModelName=distilbert_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184749-9411-495d-a8b2-5518ae4971c9",
   "metadata": {},
   "source": [
    "### Create a new model for Hugging Face roberta model with entry point script helper function\n",
    "To deploy the models in one container, we will use the Hugging Face prebuilt container which has the required packages for transformer models. However, we will use a custom entry point script for each of the model and define our own data preprocessing function. The model package structure will be similar to the one that was created for the distilbert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7194a29-84d5-4751-8852-47babfe9b5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_artifact_path = Path(\"model_artifacts\")\n",
    "local_artifact_path.mkdir(exist_ok=True, parents=True)\n",
    "model_tar_name = 'model_roberta_script.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f0e0e-bfc0-44ae-9029-380de3ddfc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.copytree('./model_token_roberta', local_artifact_path, dirs_exist_ok=True) \n",
    "shutil.copytree('./code', local_artifact_path / 'code', dirs_exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c07896-3795-4796-9b70-618d4cd40076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar_size = utils.create_tar(model_tar_name, local_artifact_path)\n",
    "print(f\"Created {model_tar_name}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19846404-f276-4d7a-81f8-132f9034691d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_path = s3_path_join(\"s3://\",bucket,prefix+\"/models\")\n",
    "model_roberta_script_uri =S3Uploader.upload(model_tar_name, model_data_path)\n",
    "print(f\"Uploaded roberta script model to {model_roberta_script_uri}\")\n",
    "%store model_roberta_script_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5722ab2-cf41-4a92-8b89-e0fc89afae47",
   "metadata": {},
   "source": [
    "#### Create the Roberta script model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8574f6-3797-46a5-9645-d9b1a013cf03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now_roberta_script = f'{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "roberta_script_model_name = f\"hf-pytorch-model-roberta-script-{now_roberta_script}\"\n",
    "print(f\"Model name : {roberta_script_model_name}\")\n",
    "%store roberta_script_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a97f28-9447-4ad9-ac06-faa1d4498303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_roberta_script = image_uris.retrieve(\n",
    "    framework=framework,\n",
    "    base_framework_version=ml_framework.lower() + framework_version,\n",
    "    region=region,\n",
    "    version=transformer_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=\"ml.c\",\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_roberta_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea8ba2-577d-4902-90ae-8adea41e7ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primary_container_roberta_script = {\n",
    "    'Image': inference_image_roberta_script,\n",
    "    'ModelDataUrl': model_roberta_script_uri\n",
    "}\n",
    "\n",
    "create_model_roberta_script_respose = sm_client.create_model(\n",
    "    ModelName=roberta_script_model_name, \n",
    "    ExecutionRoleArn=role, \n",
    "    PrimaryContainer=primary_container_roberta_script\n",
    ")\n",
    "\n",
    "print(f\"Model arn : {create_model_roberta_script_respose['ModelArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b461b-b65f-4f94-9bc5-f4360e901097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
