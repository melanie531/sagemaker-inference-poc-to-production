{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad718ed6-890f-4a3d-a6c4-9eb29d8f92e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Moving from PoC to Production in SageMaker to deploy models\n",
    "\n",
    "Moving from PoC to production to deploy models at high performance and low cost can be challenging. To help facilitate your move,SageMaker offers several features to test models and gradually move between models to production. In this notebook we will setup the necessary SageMaker resources(such as dependecies,model etc.) that will be used later in the following notebooks. Primarily there are 3 offerings that we'll look at -\n",
    "\n",
    "1. A/B Testing with SageMaker variants\n",
    "2. Deployment guardrails \n",
    "    - Blue/green deployments\n",
    "        - All At Once Traffic Shifting\n",
    "        - Canary Traffic Shifting\n",
    "        - Linear Traffic Shifting\n",
    "3. Shadow Testing\n",
    "\n",
    "Furthermore, we'll look at Load testing and Inference Recommender tool that is offered by SageMaker.\n",
    "\n",
    "***\n",
    "This notebooks is designed to run on `Python 3 (Data Science 2.0)` kernel in Amazon SageMaker Studio\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce61ca-dfb0-455f-afb4-9a28fac22b84",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's set up some required imports and basic initial variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bc46aed-816f-4021-998e-c4732bcf2d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers ipywidgets sagemaker torch -q --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96189bf5-9d67-4da7-88bd-9b33aa183225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "import time\n",
    "import os, sys\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, image_uris\n",
    "import shutil\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import torch\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# p = os.path.abspath('..')\n",
    "# if p not in sys.path:\n",
    "#     sys.path.append(p)\n",
    "import utils\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sm_session.boto_region_name\n",
    "bucket = sm_session.default_bucket()\n",
    "sm_client = sm_session.sagemaker_client\n",
    "sm_runtime = sm_session.sagemaker_runtime_client\n",
    "prefix = \"sagemaker/huggingface-pytorch-sentiment-analysis\"\n",
    "time_now = f'{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ff699-d9fb-47ba-8a6d-c3d24c06b7c7",
   "metadata": {},
   "source": [
    "### Useful objects and variables\n",
    "Common objects to interact with SageMaker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f90327-aab1-40d2-aef1-99f22098271a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'deploy_instance_type' (str)\n",
      "Stored 'model_package_group_name' (str)\n",
      "us-west-2\n",
      "arn:aws:iam::757967535041:role/service-role/AmazonSageMaker-ExecutionRole-20211027T114497\n",
      "sagemaker-us-west-2-757967535041\n"
     ]
    }
   ],
   "source": [
    "sm_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "sm_client = sm_session.sagemaker_client\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "prefix = \"sagemaker/huggingface-pytorch-sentiment-analysis\"\n",
    "deploy_instance_type = \"ml.m5.xlarge\"\n",
    "%store deploy_instance_type\n",
    "\n",
    "# The name of the Model Package Group in Amazon SageMaker Model Registry\n",
    "model_package_group_name = \"HuggingFaceModels\"\n",
    "%store model_package_group_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bf03c-2c80-49c6-9125-c42e3fd55c86",
   "metadata": {},
   "source": [
    "## Step 0: Download HuggingFace Transformer models and Create SageMaker models\n",
    "\n",
    "#### twitter-roberta-base-sentiment Pretrained Model\n",
    "\n",
    "In this example we are downloading a pre-trained HuggingFace model - `twitter-roberta-base-sentiment` from the HuggingFace library. We will use this model for classifying the text as `Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ee92bb-6173-4c4d-a7ad-d17a24cf03b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0815d39ca5264b4d85d1160617d6dd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)lve/main/config.json', max=747.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe6d6c367d54cc096a3904cf1184f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading pytorch_model.bin', max=498679497.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f5d77a4f594ab782ed5d52ec6bfd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/vocab.json', max=898822.0, style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592e1b120ff94db4a9ed486c79aba938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/merges.txt', max=456318.0, style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3251d21e5a4270b5c29ae9d06c7472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)cial_tokens_map.json', max=150.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model_token_roberta/tokenizer_config.json',\n",
       " 'model_token_roberta/special_tokens_map.json',\n",
       " 'model_token_roberta/vocab.json',\n",
       " 'model_token_roberta/merges.txt',\n",
       " 'model_token_roberta/added_tokens.json',\n",
       " 'model_token_roberta/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_MODEL_ROBERTA = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_ROBERTA)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ROBERTA)\n",
    "model.save_pretrained(\"model_token_roberta\")\n",
    "tokenizer.save_pretrained(\"model_token_roberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e56457-ff5c-434c-adb2-2d4974040743",
   "metadata": {},
   "source": [
    "### #Package the saved model to tar.gz format\n",
    "Once the model is downloaded, we need to package (tokenizer and model weights) it to `.tar.gz` format as expected by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1718ea4f-0051-4e6f-9029-282ae7eb15d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_token_roberta/vocab.json: 100%|██████████| 7/7 [00:28<00:00,  4.13s/files]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model_roberta.tar.gz, size 464.05 MB\n"
     ]
    }
   ],
   "source": [
    "tar_file_roberta = \"model_roberta.tar.gz\"\n",
    "tar_size = utils.create_tar(tar_file_roberta, Path(\"model_token_roberta\"))\n",
    "print(f\"Created {tar_file_roberta}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0476c0-47d3-4ba6-8dc9-61ae7ace09c8",
   "metadata": {},
   "source": [
    "#### Download distilbert-base-uncased-finetuned-sst-2-english by initiating a `Huggingface pipeline`\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the [task summary](https://huggingface.co/transformers/task_summary.html) for examples of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d2869b-baf6-4d31-88d6-050263dc3e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_MODEL_DISTILBERT = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "HF_TASK = \"sentiment-analysis\"\n",
    "local_artifact_path = Path(\"model_token_distilbert\")\n",
    "local_artifact_path.mkdir(exist_ok=True, parents=True)\n",
    "tar_file_distilbert = \"model_distilbert.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc00f35-1bb5-4c4c-81af-cb80072b10d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(HF_TASK, model=HF_MODEL_DISTILBERT)\n",
    "sentiment_analysis.save_pretrained(local_artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63235936-2123-4bee-a8d1-6b564e787665",
   "metadata": {},
   "source": [
    "#### Write the Inference Script\n",
    "\n",
    "To deploy a pretrained `PyTorch` model, you'll need to use the `PyTorch` estimator object to create a `PyTorchModel` object and set a different `entry_point`.\n",
    "\n",
    "You'll use the `PyTorchModel` object to deploy a `PyTorchPredictor`. This creates a `SageMaker` Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of `model_fn` is required for inference script. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `model_fn` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "Here's an example of the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc933145-db00-4247-b47b-f51b79f1a038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sagemaker-inference-poc-to-production\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7d1e6bf-267b-4f44-9f1c-5340930d35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from transformers import pipeline\n",
      "import json\n",
      "\n",
      "CSV_CONTENT_TYPE = 'text/csv'\n",
      "JSON_CONTENT_TYPE = 'application/json'\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    sentiment_analysis = pipeline(\n",
      "        \"sentiment-analysis\",\n",
      "        model=model_dir,\n",
      "        tokenizer=model_dir,\n",
      "        return_all_scores=True\n",
      "    )\n",
      "    return sentiment_analysis\n",
      "\n",
      "\n",
      "def input_fn(serialized_input_data, content_type=CSV_CONTENT_TYPE):\n",
      "    if content_type == CSV_CONTENT_TYPE:\n",
      "        input_data = serialized_input_data.splitlines()\n",
      "        return input_data\n",
      "    elif content_type == JSON_CONTENT_TYPE:\n",
      "        data = json.loads(serialized_input_data)\n",
      "        input_data = data.pop(\"inputs\", data)\n",
      "        return input_data\n",
      "    else:\n",
      "        raise Exception('Requested unsupported ContentType in Accept: ' + content_type)\n",
      "        return\n",
      "\n",
      "\n",
      "def predict_fn(input_data, model):\n",
      "    return model(input_data)\n"
     ]
    }
   ],
   "source": [
    "!cat code/inference.py  # uncomment this line of code to see the details in the py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5588e608-5e4c-4a0c-9852-ab90c6ede378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers"
     ]
    }
   ],
   "source": [
    "cat code/requirements.txt  # uncomment this line to show the packages defined in the requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f05650-6836-4ad8-a2f4-bfe2d1e66369",
   "metadata": {},
   "source": [
    "#### Create the directory structure for your model files\n",
    "\n",
    "The directory structure where you saved your PyTorch model should look something like the following:\n",
    "\n",
    "```\n",
    "|   model\n",
    "|        |--pytorch_model.bin\n",
    "|        |--config.json\n",
    "|        |--vocab.txt\n",
    "|        |--tokenizer.json\n",
    "|        |--tokenizer_config.json\n",
    "|        |--special_tokens_map.json\n",
    "|\n",
    "|        code\n",
    "|            |--inference.py\n",
    "|            |--requirements.txt\n",
    "```\n",
    "\n",
    "Where `requirements.txt` is an optional file that specifies dependencies on third-party libraries.\n",
    "\n",
    "#### Copy code to the model directory and tar the model and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a57c2083-2f57-4871-8405-0d634c2422a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.10 (default, Jun  4 2021, 14:48:32) \\n[GCC 7.5.0]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91b03198-0112-4598-9a0d-8cfa9811e3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_token_distilbert/code/inference.py: 100%|██████████| 8/8 [00:14<00:00,  1.87s/files]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model_distilbert.tar.gz, size 247.31 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info >= (3, 8):\n",
    "    shutil.copytree(\"code\", \"model_token_distilbert/code\", dirs_exist_ok=True)\n",
    "else:\n",
    "    shutil.copytree(\"code\", \"model_token_distilbert/code\")\n",
    "    \n",
    "tar_size =utils.create_tar(tar_file_distilbert, local_artifact_path)\n",
    "print(f\"Created {tar_file_distilbert}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50660e2c-8463-49fb-be66-40aa13e6dbf6",
   "metadata": {},
   "source": [
    "#### Upload the model to S3\n",
    "\n",
    "We now have the model archives ready. We need to upload them to S3 before we can use them for hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f73db5f-e0ac-4b2e-a0b1-56b0feb82db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Models to s3://sagemaker-us-west-2-757967535041/sagemaker/huggingface-pytorch-sentiment-analysis/models\n",
      "Uploaded roberta model to s3://sagemaker-us-west-2-757967535041/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_roberta.tar.gz\n",
      "Uploaded distilbert model to s3://sagemaker-us-west-2-757967535041/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_distilbert.tar.gz\n",
      "Stored 'model_data_path' (str)\n",
      "Stored 'model_roberta_uri' (str)\n",
      "Stored 'model_distilbert_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "model_data_path = s3_path_join(\"s3://\", bucket, prefix + \"/models\")\n",
    "print(f\"Uploading Models to {model_data_path}\")\n",
    "model_roberta_uri = S3Uploader.upload(\"model_roberta.tar.gz\", model_data_path)\n",
    "print(f\"Uploaded roberta model to {model_roberta_uri}\")\n",
    "model_distilbert_uri = S3Uploader.upload(\"model_distilbert.tar.gz\", model_data_path)\n",
    "print(f\"Uploaded distilbert model to {model_distilbert_uri}\")\n",
    "%store model_data_path\n",
    "%store model_roberta_uri\n",
    "%store model_distilbert_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ee584-ece9-4b02-8c74-00e41927f49f",
   "metadata": {},
   "source": [
    "#### Prebuilt HuggingFace DLC\n",
    "You can choose to use a prebuilt HuggingFace DLC as the inference image, which has the [SageMaker huggingface inference toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) for serving 🤗 Transformers models on Amazon SageMaker. The inference toolkit leverages the pipeline for the transformer library to allow zero-code deployments of models, without requiring any code for pre- or post-processing. (see more information of the default [handler service](https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/src/sagemaker_huggingface_inference_toolkit/handler_service.py) provided bythe inference toolkit).\n",
    "\n",
    "In addition to zero-code deployment, the Inference Toolkit supports \"bring your own code\" methods, where you can override the default methods. You can learn more about \"bring your own code\" in the documentation [here](https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules). In the second lab section, we will use the bring your own code method to deploy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f4df9a-af96-4078-8dcf-b564765ff6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "framework = \"huggingface\"\n",
    "transformer_version = \"4.17.0\"\n",
    "py_version = \"py38\"\n",
    "instance_type = \"ml.g\"\n",
    "image_scope = \"inference\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.10.2\"\n",
    "\n",
    "inference_image_roberta = image_uris.retrieve(\n",
    "    framework=framework,\n",
    "    base_framework_version=ml_framework.lower() + framework_version,\n",
    "    region=region,\n",
    "    version=transformer_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=instance_type,\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410f067-8dbb-403a-9090-3bb24cc8afe5",
   "metadata": {},
   "source": [
    "#### Prebuilt Pytorch DLC\n",
    "You can also use a SageMaker prebuilt [Pytorch DLC](https://github.com/aws/deep-learning-containers/tree/master/pytorch) to deploy the huggingface model. In this case, as the prebuilt Pytorch container doesn't have the transformer package, we have provided a `requirements.txt` file with the additional packages that are required to be installed to the container in the model package. See section [Create the directory structure for your model files](#Create-the-directory-structure-for-your-model-files). We also included the `inference.py` file to define the necessary functions for model loading and model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b759444c-e12e-494c-95e7-de79ac0bcb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.10.2-gpu-py38\n"
     ]
    }
   ],
   "source": [
    "inference_image_distilbert = image_uris.retrieve(\n",
    "    framework=ml_framework.lower(),\n",
    "    region=region,\n",
    "    version=framework_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=instance_type,\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20165a55-e170-404f-a2df-8bde21b6d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08-18-21-08\n",
      "Model name : hf-pytorch-model-roberta-2023-05-08-18-21-08\n",
      "Stored 'roberta_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# provide the consistent time stamp for model, endpoint config and endpoint\n",
    "now_roberta = f\"{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(now_roberta)\n",
    "roberta_model_name = f\"hf-pytorch-model-roberta-{now_roberta}\"\n",
    "print(\"Model name : {}\".format(roberta_model_name))\n",
    "%store roberta_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56bfea9c-a48d-4c66-8ed0-991d7a1c58b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primary_container = {\n",
    "    'Image': inference_image_roberta,\n",
    "    'ModelDataUrl': model_roberta_uri\n",
    "}\n",
    "create_roberta_model_response = sm_client.create_model(\n",
    "    ModelName=roberta_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b047bf9-000c-4c13-a693-2ddb773dccc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08-18-21-10\n",
      "Model name : hf-pytorch-model-distilbert-2023-05-08-18-21-10\n",
      "Stored 'distilbert_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "now_distilbert = f\"{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(now_distilbert)\n",
    "distilbert_model_name = f\"hf-pytorch-model-distilbert-{now_distilbert}\"\n",
    "print(\"Model name : {}\".format(distilbert_model_name))\n",
    "%store distilbert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b877adaa-87c7-46ce-a564-e39fe2306182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primary_container = {\n",
    "    'Image': inference_image_distilbert,\n",
    "    'ModelDataUrl': model_distilbert_uri\n",
    "}\n",
    "create_roberta_model_response = sm_client.create_model(\n",
    "    ModelName=distilbert_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184749-9411-495d-a8b2-5518ae4971c9",
   "metadata": {},
   "source": [
    "### Create a new model for Hugging Face roberta model with entry point script helper function\n",
    "To deploy the models in one container, we will use the Hugging Face prebuilt container which has the required packages for transformer models. However, we will use a custom entry point script for each of the model and define our own data preprocessing function. The model package structure will be similar to the one that was created for the distilbert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7194a29-84d5-4751-8852-47babfe9b5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_artifact_path = Path(\"model_artifacts\")\n",
    "local_artifact_path.mkdir(exist_ok=True, parents=True)\n",
    "model_tar_name = 'model_roberta_script.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "406f0e0e-bfc0-44ae-9029-380de3ddfc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sys.version_info >= (3, 8):\n",
    "    shutil.copytree('./model_token_roberta', local_artifact_path, dirs_exist_ok=True) \n",
    "    shutil.copytree('./code', local_artifact_path / 'code', dirs_exist_ok=True) \n",
    "else:\n",
    "    shutil.copytree('./model_token_roberta', local_artifact_path)\n",
    "    shutil.copytree('./code', local_artifact_path / 'code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6c07896-3795-4796-9b70-618d4cd40076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_artifacts/code/inference.py: 100%|██████████| 9/9 [00:27<00:00,  3.03s/files]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model_roberta_script.tar.gz, size 464.05 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tar_size = utils.create_tar(model_tar_name, local_artifact_path)\n",
    "print(f\"Created {model_tar_name}, size {tar_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19846404-f276-4d7a-81f8-132f9034691d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded roberta script model to s3://sagemaker-us-west-2-757967535041/sagemaker/huggingface-pytorch-sentiment-analysis/models/model_roberta_script.tar.gz\n",
      "Stored 'model_roberta_script_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "model_data_path = s3_path_join(\"s3://\",bucket,prefix+\"/models\")\n",
    "model_roberta_script_uri =S3Uploader.upload(model_tar_name, model_data_path)\n",
    "print(f\"Uploaded roberta script model to {model_roberta_script_uri}\")\n",
    "%store model_roberta_script_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5722ab2-cf41-4a92-8b89-e0fc89afae47",
   "metadata": {},
   "source": [
    "#### Create the Roberta script model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d8574f6-3797-46a5-9645-d9b1a013cf03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name : hf-pytorch-model-roberta-script-2023-05-08-18-37-59\n",
      "Stored 'roberta_script_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "now_roberta_script = f'{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "roberta_script_model_name = f\"hf-pytorch-model-roberta-script-{now_roberta_script}\"\n",
    "print(f\"Model name : {roberta_script_model_name}\")\n",
    "%store roberta_script_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8a97f28-9447-4ad9-ac06-faa1d4498303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "inference_image_roberta_script = image_uris.retrieve(\n",
    "    framework=framework,\n",
    "    base_framework_version=ml_framework.lower() + framework_version,\n",
    "    region=region,\n",
    "    version=transformer_version,\n",
    "    py_version=py_version,\n",
    "    instance_type=\"ml.c\",\n",
    "    image_scope=image_scope,\n",
    ")\n",
    "\n",
    "print(inference_image_roberta_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17ea8ba2-577d-4902-90ae-8adea41e7ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model arn : arn:aws:sagemaker:us-west-2:757967535041:model/hf-pytorch-model-roberta-script-2023-05-08-18-37-59\n"
     ]
    }
   ],
   "source": [
    "primary_container_roberta_script = {\n",
    "    'Image': inference_image_roberta_script,\n",
    "    'ModelDataUrl': model_roberta_script_uri\n",
    "}\n",
    "\n",
    "create_model_roberta_script_respose = sm_client.create_model(\n",
    "    ModelName=roberta_script_model_name, \n",
    "    ExecutionRoleArn=role, \n",
    "    PrimaryContainer=primary_container_roberta_script\n",
    ")\n",
    "\n",
    "print(f\"Model arn : {create_model_roberta_script_respose['ModelArn']}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
